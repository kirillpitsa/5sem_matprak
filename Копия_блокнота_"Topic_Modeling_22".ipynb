{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMEFaY4svMpR",
        "outputId": "88a90b0a-a613-44d4-8ebd-c73263e3ab45"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_text(text, lemmataizer):\n",
        "    tokens = text.split()\n",
        "    return ' '.join(map(lambda w: lemmatizer.lemmatize(w), tokens))\n",
        "\n",
        "def myLDA(n_dk, n_kw, n_k, r_th, text, word, n_topic, alpha = 1, beta = 1, max_iter=10):\n",
        "    for i in tqdm(range(max_iter)):\n",
        "        for j in range(len(text)):\n",
        "            #вычитаем из счетчиков по 1\n",
        "            n_dk[text[j], r_th[j]] -= 1\n",
        "            n_kw[r_th[j], word[j]] -= 1\n",
        "            n_k[r_th[j]] -= 1\n",
        "            #вычисляем pk-ые\n",
        "            p = (n_dk[text[j], :] + alpha) * (n_kw[:, word[j]] + beta) / (n_k + X_train.shape[1])\n",
        "\n",
        "            #новая тема по распределению\n",
        "            r_th[j] = np.random.choice(np.arange(n_topic), p = p / p.sum())\n",
        "\n",
        "            #прибавляем счетчикам по 1\n",
        "            n_dk[text[j], r_th[j]] += 1\n",
        "            n_kw[r_th[j], word[j]] += 1\n",
        "            n_k[r_th[j]] += 1\n",
        "    #return n_dk, n_kw, n_k, r_th\n",
        "    return n_kw"
      ],
      "metadata": {
        "id": "yQvCZCgXOumI"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "                    lowercase=True, stop_words= stopwords.words(\"english\"),\n",
        "                    analyzer='word', binary=True,\n",
        "                    max_df=0.05, min_df = 0.005,\n",
        "                    token_pattern = r'(?u)\\b[a-z]{2,}\\b'\n",
        ")\n",
        "\n",
        "lemma_texts = []\n",
        "for t in tqdm(newsgroups_train.data):\n",
        "    lemma_texts.append(lemma_text(t, lemmatizer))\n",
        "\n",
        "X_train = vectorizer.fit_transform(lemma_texts).toarray()\n",
        "print('After lemma: %d'%(len(vectorizer.vocabulary_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxI--1HOvOpE",
        "outputId": "e4310712-8689-4eea-9063-8fd5f041123c"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11314/11314 [00:08<00:00, 1358.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lemma: 2187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_topic = 20\n",
        "n_topwords = 10\n",
        "n_kw = np.zeros( n_topic * X_train.shape[1]).reshape(n_topic, X_train.shape[1])\n",
        "n_dk = np.zeros( n_topic * X_train.shape[0]).reshape(X_train.shape[0], n_topic)\n",
        "n_k = np.zeros(n_topic)\n",
        "#все места, где встречаются слова в текстах\n",
        "text, word = X_train.nonzero()\n",
        "#r_th - массив случайных топиков длины len(text)\n",
        "r_th = np.random.choice(n_topic, len(text))\n",
        "for i, j, k in zip(text, word, r_th):\n",
        "    n_dk[i, k] += 1\n",
        "    n_kw[k, j] += 1\n",
        "    n_k[k] += 1"
      ],
      "metadata": {
        "id": "_1BUzmsEyzrr"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_kw  = myLDA(n_dk, n_kw, n_k, r_th, text, word, 20, 1, 1, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKb88iYhy3on",
        "outputId": "94fa4b7f-f42d-4f7a-ae77-a04c38665f89"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [24:41<00:00, 14.81s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.argsort(n_kw, axis=1)[:, -n_topwords:]\n",
        "for i in range(n_topic):\n",
        "    matrix = np.zeros(X_train.shape[1]).reshape(1, -1)\n",
        "    for j in result[i]:\n",
        "        matrix[0, j] = 1\n",
        "    print('theme {} \\t{}'.format(i + 1, '\\t'.join(vectorizer.inverse_transform(matrix)[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "253V2iZby5WU",
        "outputId": "319008c0-f212-4f45-99d0-a46acb8687cd"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theme 1 \tcause\tdoctor\teffect\tespecially\tgordon\tsoon\tsurrender\ttest\tusually\twhether\n",
            "theme 2 \taddress\tadvance\tanybody\tappreciate\tappreciated\temail\thi\tinfo\treply\tsend\n",
            "theme 3 \tamount\tcurrent\tdifference\thigher\tlarge\tlow\tlower\tnote\trate\tsmall\n",
            "theme 4 \tcard\tcomputer\tdisk\tdrive\tdriver\tmac\tmemory\tmonitor\tpc\tvideo\n",
            "theme 5 \taddress\tarchive\tarticle\tlist\tnet\torder\tposted\tposting\trequest\tsend\n",
            "theme 6 \tever\tfeel\tfree\thope\tkind\tpretty\tquite\tremember\tseem\ttrying\n",
            "theme 7 \tfan\tgame\tgames\thockey\tleague\tplay\tplayer\tseason\tteam\twin\n",
            "theme 8 \tcontrol\tcountry\tcrime\tcriminal\tgun\tlaw\tperson\tpolice\tself\tweapon\n",
            "theme 9 \tarmenian\tarmenians\taway\tcity\thistory\tkilled\tmen\ttoday\tturkish\twar\n",
            "theme 10 \tbad\tbike\tcar\tdrive\tengine\tfront\tmile\troad\tspeed\tturn\n",
            "theme 11 \tamerican\tapril\tclinton\tmoney\tplan\tpresident\tpublic\tstates\tsupport\ttoday\n",
            "theme 12 \tanyway\tdeleted\tguess\theard\toh\tok\tsorry\tstart\tstuff\twrong\n",
            "theme 13 \tapplication\tcode\tdisplay\tftp\timage\trunning\tsoftware\tversion\twindow\twindows\n",
            "theme 14 \tchip\tclipper\tdata\tencryption\tkey\tphone\tpublic\tsecret\tsecure\tsecurity\n",
            "theme 15 \tcame\thappened\tleft\tnext\tsaw\tstarted\ttold\ttook\twanted\twent\n",
            "theme 16 \tbible\tbook\tchrist\tchristian\tchristians\tchurch\tjesus\tlife\treligion\tword\n",
            "theme 17 \tagree\targument\tcannot\tcertainly\tclaim\tevidence\tmatter\tsaying\tstatement\twrong\n",
            "theme 18 \talways\tarab\tisrael\tisraeli\tjews\tlive\tpeace\tsituation\tstop\twar\n",
            "theme 19 \tcenter\tcontact\tdata\tearth\tfax\tnasa\tresearch\tscience\tspace\tuniversity\n",
            "theme 20 \tbuy\tcondition\tcost\tinterested\toffer\toriginal\tprice\tsale\tsell\tshipping\n"
          ]
        }
      ]
    }
  ]
}